{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP90wm/WywxSLIutlJ/MyRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattheweisenberg6/MAT421/blob/main/ModuleD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Introduction**\n",
        "\n",
        "Linear algebra is a mathematical field used in many disciplines. It is used heavily in data science and machine learning, especially algorithms. Linear algebra plays a central role in solving data science problems using tools like vector spaces, orthogonality, eigenvalues, matrix decomposition, and linear regression."
      ],
      "metadata": {
        "id": "rRXU8dGjeErj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Elements of Linear Algebra**\n",
        "\n",
        "Linear Combinations\n",
        "\n",
        "Linear combinations are new vectors made by multiplying each vector by a constant then multiplying the result. The formal definition of a linear subspace is a subset U ⊆ V closeed under vector addition and vector multiplication. The span of a set of vectors also classifies as a linear subspace.\n",
        "\n",
        "Orthogonality\n",
        "\n",
        "We can use orthonormal bases in Python to simplify mathematical representations and learn more about the problem we want to solve. A list of vectors is orthonormal if u_i vectors are pairwise orthogonal with a norm of 1 each. We can define the inner product of two vectors x and y with the equation, <x,y> = x·y = ∑n x_i*y_i and ∥x∥ =√ ∑n (x_i) ^ 2. If we have X ⊆ Y as a linear subspace  with orthonormal basis (q1,...q_k), then we can define orthonormal projection of x ∈ X on Y to confirm the optimality of the vectors for the best approximation of orthonormality.\n",
        "\n",
        "Gram-Schmidt Process\n",
        "\n",
        "Gram-Schmidt algorithm is used to find the orthonormal basis. A vector x_i is added each time after the previously used orthonormal projection is removed. The states that for (x_1,...x_k) in R_n that is linearly independent, there must be an orthonormal basis such that (y_1,...y_k) has span(x_1,...x_k).\n",
        "\n",
        "Eigenvectors and Eigenvalues\n",
        "\n",
        "Given the equation: Ax = λx,\n",
        "Where A is an n×n matrix, x is an n×1 column vector (x ≠ 0), and λ is a scalar. Any value of λ that satisfies this equation is called an eigenvalue of the matrix A. The corresponding vector x is known as an eigenvector associated with λ. An eigenvalue represents the factor by which an eigenvector is scaled during a transformation.\n"
      ],
      "metadata": {
        "id": "EoRx3NrQeo6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import eig\n",
        "\n",
        "#using python to calculate eigenvalue and eigenvector\n",
        "\n",
        "a = np.array([[2, 2, 4],\n",
        "              [7, 5, 20],\n",
        "              [3, 6, 9]])\n",
        "\n",
        "# eigenvalue function call\n",
        "w,v=eig(a)\n",
        "print('Eigenvalue:', w)\n",
        "print('Eigenvector', v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kuF2Oj6irdE",
        "outputId": "ff41c343-f37c-4d2f-83e3-1ccee4eb15d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalue: [19.59961085  0.58521612 -4.18482697]\n",
            "Eigenvector [[ 0.21315557  0.94464188  0.03108826]\n",
            " [ 0.82315793 -0.01250217 -0.91239496]\n",
            " [ 0.52628482 -0.32786494  0.4081286 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Linear Regression**\n",
        "\n",
        "Linear regression models rely on a linear relationship with their unknown parameters, making them generally easier to fit compared to non-linear models. The linear least squares problem can be efficiently solved using QR decomposition, which is derived through the Gram-Schmidt process to form an orthonormal basis. This decomposition is expressed as A = QR. Python provides convenient tools to compute A, Q, and R in the decomposition."
      ],
      "metadata": {
        "id": "Mo9kDVPijD_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "import numpy\n",
        "import scipy.linalg\n",
        "\n",
        "A = numpy.array([[7, 42, 8],\n",
        "                  [-12, 29, -78],\n",
        "                  [25, 121, -40]])\n",
        "Q, R = numpy.linalg.qr(A) # decomposing array A\n",
        "\n",
        "print(\"A\")\n",
        "pprint.pprint(A)\n",
        "\n",
        "print(\"Q\")\n",
        "pprint.pprint(Q)\n",
        "\n",
        "print(\"R:\")\n",
        "pprint.pprint(R)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBuVMd2oji7U",
        "outputId": "cbc78d24-a182-414d-fe08-cc778a251a04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "array([[  7,  42,   8],\n",
            "       [-12,  29, -78],\n",
            "       [ 25, 121, -40]])\n",
            "Q\n",
            "array([[-0.24474926, -0.20630898, -0.94738292],\n",
            "       [ 0.41957016, -0.90341392,  0.08834117],\n",
            "       [-0.8741045 , -0.37587217,  0.30767098]])\n",
            "R:\n",
            "array([[ -28.60069929, -103.87857897,    0.27971344],\n",
            "       [   0.        ,  -80.34451339,   83.85070095],\n",
            "       [   0.        ,    0.        ,  -26.77651415]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least squares problems can be solved using a similar approach, applying the least squares regression formula β = (AᵀA)⁻¹AᵀY. This method provides a solution when directly computing the matrix inverse to solve Ax = b is not feasible."
      ],
      "metadata": {
        "id": "7udobVNakAUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.linspace(0, 5, 50)\n",
        "y = 2 + 3 * x + 2 * np.random.random(len(x)) # random x and y values\n",
        "\n",
        "A = np.vstack([x, np.ones(len(x))]).T\n",
        "y = y[:, np.newaxis]\n",
        "\n",
        "beta = np.dot(np.linalg.inv(A.T @ A) @ A.T, y) # regression for A and T values\n",
        "print(beta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKB_926WkMXD",
        "outputId": "f32a21fc-2ad3-4275-a765-6afb6b208446"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.95973954]\n",
            " [3.22154439]]\n"
          ]
        }
      ]
    }
  ]
}